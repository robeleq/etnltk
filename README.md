# Ethiopian Natural Language Toolkit (etnltk)

- The Ethiopian Natural Language Toolkit (etnltk) project aimed to develop a suite of open source Natural Language Processing modules for the Ethiopian languages.
- The Ethiopian Natural Language Toolkit (etnltk) is built using python language and takes inspiration from `spacy` and `nltk` libraries.

## Installation

### pip

- **etnltk** supports Python 3.6 or later. We recommend that you install etnltk via `pip`, the Python package manager. To install, simply run:

  ```python
    pip install etnltk
  ```

### From Source

- Alternatively, you can also install from source via `etnltk` git repository, which will give you more flexibility in developing on top of etltk. For this option, run

  ```python
    git clone https://github.com/robeleq/etnltk.git
    
    cd etnltk
    
    pip install -e .
  ```

## Documentation

<https://etnltk.netlify.app/>

## Usage

1. Amharic text preprocessing with Amharic document
    - Preprocessing amharic text is very simple: you can simply pass the text to the `Amharic` document and access all annotations from the returned Amharic document object:

    ```python
      from etnltk import Amharic

      sample_text = """
        рѕџрІФрІЮрІФ 14рЇБ 2014 рІЊ.рѕЮ ­ЪцЌ рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх /Artificial Intelligence/ ріарѕЂріЋ ріФрѕѕрЅарЅх рІЮрЅЁрЅ░ріЏ рІ░рѕерїЃ рІѕрІ░ рѕІрЅђ рІ░рѕерїЃ рѕѕрѕЏрІхрѕерѕхрЇБ рѕЃрїѕрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрІЊрѕѕрѕЮ рЅ░рІ░рѕФрѕй рѕѕрѕЏрІхрѕерїЇрЇБ ріарїѕрѕФрІі ріарЅЁрѕЮріЋ рѕѕрѕЏрѕ│рІ░рїЇ ріЦріЊ рЅ░рїарЅЃрѕџ рѕѕрѕўрѕєріЋ рЅарїІрѕФ ріарЅЦрѕ« рѕўрѕхрѕФрЅ▒ ріЦрїЁрїЇ рїарЅЃрѕџ ріљрІЇрЇАрЇА

        рЅарѕЏрѕйріЋ рІЊрѕхрЅ░рѕЮрѕ« (Machine Learning) ріарѕЏріФріЮріљрЅх рІерїйрѕЂрЇЇ ріЊрѕЎріЊрІјрЅй рЅаріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рѕЦрѕГрІЊрЅх рѕѕрѕЏрѕ░рѕЇрїаріЋрЇБ рІерїйрѕЂрЇЇ рІ│рЅ│ріЋ рѕўрѕ░рЅЦрѕ░рЅЦ ріЦріЊ рѕЏрІ░рѕФрїђрЅхрЇц рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅйріЋ /Natural Language Processing Tools/ рЅарѕўрїарЅђрѕЮ рІерїйрѕЂрЇЇ рІ│рЅ│ріЋ рЇЋрѕ«рѕ░рѕх рѕЏрІхрѕерїЇ рЅ░рЅђрІ│рѕџ ріЦріЊ рѕўрѕ░рѕерЅ│рІі рїЅрІ│рІГ ріљрІЇрЇб
      """
  
      # Annotating Amharic document
      doc = Amharic(sample_text)

      # print the `clean` text:
      print(doc)
      
      # output: Amharic("рѕџрІФрІЮрІФ рІЊрѕўрЅ░ рѕЮрѕЁрѕерЅх рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх ріарѕЂріЋ ріФрѕѕрЅарЅх рІЮрЅЁрЅ░ріЏ рІ░рѕерїЃ рІѕрІ░ рѕІрЅђ рІ░рѕерїЃ рѕѕрѕЏрІхрѕерѕх рѕђрїѕрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕріарѕѕрѕЮ рЅ░рІ░рѕФрѕй рѕѕрѕЏрІхрѕерїЇ ріарїѕрѕФрІі ріарЅЁрѕЮріЋ рѕѕрѕЏрѕ│рІ░рїЇ ріЦріЊ рЅ░рїарЅЃрѕџ рѕѕрѕўрѕєріЋ рЅарїІрѕФ ріарЅЦрѕ« рѕўрѕхрѕФрЅ▒ ріЦрїЁрїЇ рїарЅЃрѕџ ріљрІЇ рЅарѕЏрѕйріЋ ріарѕхрЅ░рѕЮрѕ« ріарѕЏріФріЮріљрЅх рІерЇЁрѕЂрЇЇ ріЊрѕЎріЊрІјрЅй рЅаріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рѕхрѕГріарЅх рѕѕрѕЏрѕ░рѕЇрїаріЋ рІерЇЁрѕЂрЇЇ рІ│рЅ│ріЋ рѕўрѕ░рЅЦрѕ░рЅЦ ріЦріЊ рѕЏрІ░рѕФрїђрЅх рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅйріЋ рЅарѕўрїарЅђрѕЮ рІерЇЁрѕЂрЇЇ рІ│рЅ│ріЋ рЇЋрѕ«рѕ░рѕх рѕЏрІхрѕерїЇ рЅ░рЅђрІ│рѕџ ріЦріЊ рѕўрѕ░рѕерЅ│рІі рїЅрІ│рІГ ріљрІЇ")
    ```

     - Here is a another example of performing text cleaning on a piece of plaintext using `clean_amharic` function:

    ```python
    from etnltk.lang.am import (
      preprocessing,
      clean_amharic
    )

    sample_text = """
      рѕџрІФрІЮрІФ 14рЇБ 2014 рІЊ.рѕЮ ­ЪцЌ рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх /Artificial Intelligence/ ріарѕЂріЋ ріФрѕѕрЅарЅх рІЮрЅЁрЅ░ріЏ рІ░рѕерїЃ рІѕрІ░ рѕІрЅђ рІ░рѕерїЃ рѕѕрѕЏрІхрѕерѕхрЇБ рѕЃрїѕрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрІЊрѕѕрѕЮ рЅ░рІ░рѕФрѕй рѕѕрѕЏрІхрѕерїЇрЇБ ріарїѕрѕФрІі ріарЅЁрѕЮріЋ рѕѕрѕЏрѕ│рІ░рїЇ ріЦріЊ рЅ░рїарЅЃрѕџ рѕѕрѕўрѕєріЋ рЅарїІрѕФ ріарЅЦрѕ« рѕўрѕхрѕФрЅ▒ ріЦрїЁрїЇ рїарЅЃрѕџ ріљрІЇрЇАрЇА

      рЅарѕЏрѕйріЋ рІЊрѕхрЅ░рѕЮрѕ« (Machine Learning) ріарѕЏріФріЮріљрЅх рІерїйрѕЂрЇЇ ріЊрѕЎріЊрІјрЅй рЅаріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рѕЦрѕГрІЊрЅх рѕѕрѕЏрѕ░рѕЇрїаріЋрЇБ рІерїйрѕЂрЇЇ рІ│рЅ│ріЋ рѕўрѕ░рЅЦрѕ░рЅЦ ріЦріЊ рѕЏрІ░рѕФрїђрЅхрЇц рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅйріЋ /Natural Language Processing Tools/ рЅарѕўрїарЅђрѕЮ рІерїйрѕЂрЇЇ рІ│рЅ│ріЋ рЇЋрѕ«рѕ░рѕх рѕЏрІхрѕерїЇ рЅ░рЅђрІ│рѕџ ріЦріЊ рѕўрѕ░рѕерЅ│рІі рїЅрІ│рІГ ріљрІЇрЇб
    """

    # Define a custom preprocessor pipeline
    custom_pipeline = [
      preprocessing.remove_emojis, 
      preprocessing.remove_digits,
      preprocessing.remove_ethiopic_punct,
      preprocessing.remove_english_chars, 
      preprocessing.remove_punct
    ]
    
    # `clean_amharic` function takes a custom pipeline, if not uses the default pipeline
    cleaned = clean_amharic(sample_text, abbrev=False, pipeline=custom_pipeline)

    # print the `clean` text:
    print(cleaned)
    # output: рѕџрІФрІЮрІФ рІЊрѕўрЅ░ рѕЮрѕЁрѕерЅх рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх ріарѕЂріЋ ріФрѕѕрЅарЅх рІЮрЅЁрЅ░ріЏ рІ░рѕерїЃ рІѕрІ░ рѕІрЅђ рІ░рѕерїЃ рѕѕрѕЏрІхрѕерѕх рѕђрїѕрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕріарѕѕрѕЮ рЅ░рІ░рѕФрѕй рѕѕрѕЏрІхрѕерїЇ ріарїѕрѕФрІі ріарЅЁрѕЮріЋ рѕѕрѕЏрѕ│рІ░рїЇ ріЦріЊ рЅ░рїарЅЃрѕџ рѕѕрѕўрѕєріЋ рЅарїІрѕФ ріарЅЦрѕ« рѕўрѕхрѕФрЅ▒ ріЦрїЁрїЇ рїарЅЃрѕџ ріљрІЇ рЅарѕЏрѕйріЋ ріарѕхрЅ░рѕЮрѕ« ріарѕЏріФріЮріљрЅх рІерЇЁрѕЂрЇЇ ріЊрѕЎріЊрІјрЅй рЅаріарѕГрЅ▓рЇірѕ╗рѕЇ рібріЋрЅ░рѕѕрїђріЋрѕх рѕхрѕГріарЅх рѕѕрѕЏрѕ░рѕЇрїаріЋ рІерЇЁрѕЂрЇЇ рІ│рЅ│ріЋ рѕўрѕ░рЅЦрѕ░рЅЦ ріЦріЊ рѕЏрІ░рѕФрїђрЅх рІеріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅйріЋ рЅарѕўрїарЅђрѕЮ рІерЇЁрѕЂрЇЇ рІ│рЅ│ріЋ рЇЋрѕ«рѕ░рѕх рѕЏрІхрѕерїЇ рЅ░рЅђрІ│рѕџ ріЦріЊ рѕўрѕ░рѕерЅ│рІі рїЅрІ│рІГ ріљрІЇ
    ```

2. Tokenization - Sentence
    - Here is a simple example of performing sentence tokenization on a piece of plaintext using Amharic document:
    - Within Amharic document, annotations are further stored in `Sentences`

    ```python
    from etnltk import Amharic

    sample_text = """
      рІерѕЏрѕйріЋ рѕѕрѕГріњріЋрїЇ рѕхрѕЇрЅ░-рЅђрѕўрѕ«рЅй  (Algorithms) рЅарѕўрїарЅђрѕЮ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅх ріЦріЊ рѕўрѕерІ│рЅхрЇБ рІерїйрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅхрЇБ рІерЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІерѕЃрїѕрѕфріЏ ріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй (NLP tools) рЇБ рѕхрѕЇрЅ░-рЅђрѕўрѕ«рЅй ріЦріЊ рѕърІ┤рѕјрЅйріЋ рѕЏрІўрїІрїђрЅх рЅ░рїѕрЅб ріљрІЇрЇб рЅарІџрѕЁрѕЮ рѕўрѕ░рѕерЅх ріарѕЏрѕГріЏрЇБ ріарЇІріЋ рідрѕ«рѕърЇБ рѕХрѕЏрѕіріЏ ріЦріЊ рЅхрїЇрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрѕЏрѕйріЋ рІерѕЏрѕхрЅ░рѕЏрѕГ рѕѓрІ░рЅхріЋ рЅђрѕІрѕЇріЊ рІерЅ░рЅђрѕІрЅ░рЇЇ ріЦріЋрІ▓рѕєріЋ рІФрѕхрЅйрѕІрѕЇрЇАрЇА
    """

    # Annotating Amharic Text
    doc = Amharic(sample_text)

    # print all list of `Sentence` in a document:
    print(doc.sentences)
    # output: [Sentence("рІерѕЏрѕйріЋ рѕѕрѕГріњріЋрїЇ рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй рЅарѕўрїарЅђрѕЮ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅх ріЦріЊ рѕўрѕерІ│рЅх рІерЇЁрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅх рІерЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІерѕђрїѕрѕфріЏ ріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй ріЦріЊ рѕърІ┤рѕјрЅйріЋ рѕЏрІўрїІрїђрЅх рЅ░рїѕрЅб ріљрІЇ"), Sentence("рЅарІџрѕЁрѕЮ рѕўрѕ░рѕерЅх ріарѕЏрѕГріЏ ріарЇІріЋ рідрѕ«рѕъ рѕХрѕЏрѕіріЏ ріЦріЊ рЅхрїЇрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрѕЏрѕйріЋ рІерѕЏрѕхрЅ░рѕЏрѕГ рѕѓрІ░рЅхріЋ рЅђрѕІрѕЇріЊ рІерЅ░рЅђрѕІрЅ░рЇЇ ріЦріЋрІ▓рѕєріЋ рІФрѕхрЅйрѕІрѕЇ")]
    ```

    - Here is another example of performing sentence tokenization on a piece of plaintext using `sentence_tokenize` function:

    ```python
    from etnltk.tokenize.am import sent_tokenize

    sample_text = """
      рІерѕЏрѕйріЋ рѕѕрѕГріњріЋрїЇ рѕхрѕЇрЅ░-рЅђрѕўрѕ«рЅй  (Algorithms) рЅарѕўрїарЅђрѕЮ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅх ріЦріЊ рѕўрѕерІ│рЅхрЇБ рІерїйрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅхрЇБ рІерЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІерѕЃрїѕрѕфріЏ ріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй (NLP tools) рЇБ рѕхрѕЇрЅ░-рЅђрѕўрѕ«рЅй ріЦріЊ рѕърІ┤рѕјрЅйріЋ рѕЏрІўрїІрїђрЅх рЅ░рїѕрЅб ріљрІЇрЇб рЅарІџрѕЁрѕЮ рѕўрѕ░рѕерЅх ріарѕЏрѕГріЏрЇБ ріарЇІріЋ рідрѕ«рѕърЇБ рѕХрѕЏрѕіріЏ ріЦріЊ рЅхрїЇрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрѕЏрѕйріЋ рІерѕЏрѕхрЅ░рѕЏрѕГ рѕѓрІ░рЅхріЋ рЅђрѕІрѕЇріЊ рІерЅ░рЅђрѕІрЅ░рЇЇ ріЦріЋрІ▓рѕєріЋ рІФрѕхрЅйрѕІрѕЇрЇАрЇА
    """

    # Annotating a Document
    sentences = sent_tokenize(sample_text)

    # print all list of sentence:
    print(sentences)
    # output: ['рІерѕЏрѕйріЋ рѕѕрѕГріњріЋрїЇ рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй рЅарѕўрїарЅђрѕЮ рЅІріЋрЅІрІјрЅйріЋ рѕўрѕѕрІерЅх ріЦріЊ рѕўрѕерІ│рЅх рІерЇЁрѕЂрЇЇ рІГрІўрЅХрЅйріЋ рѕўрѕѕрІерЅх рІерЅІріЋрЅІріЋ рѕўрІІрЅЁрѕГ рѕўрЅ░ріЋрЅ░ріЋ рІерѕџрІФрѕхрЅйрѕЅ рІерѕђрїѕрѕфріЏ ріЊрЅ╣рѕФрѕЇ рѕІріЋрїЅрІїрїЁ рЇЋрѕ«рѕ░рѕ▓ріЋрїЇ рЅ▒рѕјрЅй рѕхрѕЇрЅ░рЅђрѕўрѕ«рЅй ріЦріЊ рѕърІ┤рѕјрЅйріЋ рѕЏрІўрїІрїђрЅх рЅ░рїѕрЅб ріљрІЇ', 'рЅарІџрѕЁрѕЮ рѕўрѕ░рѕерЅх ріарѕЏрѕГріЏ ріарЇІріЋ рідрѕ«рѕъ рѕХрѕЏрѕіріЏ ріЦріЊ рЅхрїЇрѕГріЏ рЅІріЋрЅІрІјрЅйріЋ рѕѕрѕЏрѕйріЋ рІерѕЏрѕхрЅ░рѕЏрѕГ рѕѓрІ░рЅхріЋ рЅђрѕІрѕЇріЊ рІерЅ░рЅђрѕІрЅ░рЇЇ ріЦріЋрІ▓рѕєріЋ рІФрѕхрЅйрѕІрѕЇ']

3. Tokenization - Word
    - Here is a simple example of performing word tokenization on a piece of plaintext using AmharicDocument:
    - Within Amharic focument, annotations are further stored in `Words`.

    ```python
    from etnltk import AmharicDocument

    sample_text = """
      РђюрЅ░рѕеріЏрЇБ рЅ░рѕеріЏ!РђЮ ріарѕѕ ріљрѕГрѕ▒рЇб рІѕрІГрІўрѕ«
      рЅ│рѕфрі│рЇБ РђюріарЅцрЅх!РђЮ рЅЦрѕѕрІЇ рІерѕЂрѕѕрЅх
      рІЊрѕўрЅх рѕЇрїЃрЅИрІЇріЋ рІГрІўрІЇ рїѕрЅАрЇб
      РђюрѕЮріЉріЋ ріљрІЇ рІФрѕўрѕўрІЇ?РђЮ рІХріГрЅ░рѕ»
      рїарІерЅЂрЇб РђюріарІФрІЕрЅхрѕЮ! рЇђрїЅрѕЕ рѕ│рѕхрЅирѕЇрЇц
      рѕєрІ▒ рЅ░ріљрЇЇрЅирѕЇрЇц рІхрІ▒рѕЮ рІГрІ░рѕЏрѕЇРђЮ
      ріарѕЅ рІѕрІГрІўрѕ« рЅ│рѕфрі│рЇб рІХріГрЅ░рѕ»рѕЮрЇБ
      РђюрЅарїБрѕЮ рІФрѕ│рІЮріЊрѕЇрЇц ріЦріЋрІ░рІџрѕЁ
      рІФрІ░рѕерїѕрІЇ рІерЅ░рѕўрїБрїаріљ рѕЮрїЇрЅЦ ріарѕѕрѕЏрїЇріўрЅ▒ ріљрІЇрЇб ріарѕЂріЋрѕЮ рІѕрЅ░рЅхрЇБ
      ріЦріЋрЅЂрѕІрѕЇрЇБ рѕЏрѕГрЇБ ріарЅхріГрѕЇрЅхріЊ рЇЇрѕФрЇЇрѕг рІГрѕўрїЇрЅАрЅхрЇц рЅХрѕј рІГрѕ╗рѕѕрІІрѕЇрЇц
      рѕѕріарѕЂріЉ рїЇріЋ рѕўрІхріЃріњрЅх ріарІЮрѕѕрЅ│рѕѕрѕЂРђЮ рЅарѕЏрѕѕрЅх ріарѕхрѕерІирЅИрІЇрЇб рІѕрІГрІўрѕ«
      рЅ│рѕфрі│рѕЮ РђюрІѕрІГ ріарѕѕрѕЏрІѕрЅЁ! рѕЇрїёріЋ рЅарѕЮрїЇрЅЦ ріЦрїЦрѕерЅх рїѕрІхрІгрІЇ ріљрЅарѕГ"
      рЅарѕЏрѕѕрЅх ріарѕѕрЅђрѕ▒рЇб

      """
    
    # Annotating Amharic Text
    doc = Amharic(sample_text)

    # print all list of `AmharicWord` in a document:
    print(doc.words)
    # output: ['рЅ░рѕеріЏ', 'рЅ░рѕеріЏ', 'ріарѕѕ', 'ріљрѕГрѕ▒', 'рІѕрІГрІўрѕ«', 'рЅ│рѕфрі│', 'ріарЅцрЅх', 'рЅЦрѕѕрІЇ', 'рІерѕЂрѕѕрЅх', 'ріарѕўрЅх', 'рѕЇрїЃрЅИрІЇріЋ', 'рІГрІўрІЇ', 'рїѕрЅА', 'рѕЮріЉріЋ', 'ріљрІЇ', 'рІФрѕўрѕўрІЇ', 'рІХріГрЅ░рѕ»', 'рїарІерЅЂ', 'ріарІФрІЕрЅхрѕЮ', 'рЇђрїЅрѕЕ', 'рѕ│рѕхрЅирѕЇ', 'рѕєрІ▒', 'рЅ░ріљрЇЇрЅирѕЇ', 'рІхрІ▒рѕЮ', 'рІГрІ░рѕЏрѕЇ', 'ріарѕЅ', 'рІѕрІГрІўрѕ«', 'рЅ│рѕфрі│', 'рІХріГрЅ░рѕ»рѕЮ', 'рЅарїБрѕЮ', 'рІФрѕ│рІЮріЊрѕЇ', 'ріЦріЋрІ░рІџрѕЁ', 'рІФрІ░рѕерїѕрІЇ', 'рІерЅ░рѕўрїБрїаріљ', 'рѕЮрїЇрЅЦ', 'ріарѕѕрѕЏрїЇріўрЅ▒', 'ріљрІЇ', 'ріарѕЂріЋрѕЮ', 'рІѕрЅ░рЅх', 'ріЦріЋрЅЂрѕІрѕЇ', 'рѕЏрѕГ', 'ріарЅхріГрѕЇрЅхріЊ', 'рЇЇрѕФрЇЇрѕг', 'рІГрѕўрїЇрЅАрЅх', 'рЅХрѕј', 'рІГрѕ╗рѕѕрІІрѕЇ', 'рѕѕріарѕЂріЉ', 'рїЇріЋ', 'рѕўрІхрѕђріњрЅх', 'ріарІЮрѕѕрЅ│рѕѕрѕЂ', 'рЅарѕЏрѕѕрЅх', 'ріарѕхрѕерІирЅИрІЇ', 'рІѕрІГрІўрѕ«', 'рЅ│рѕфрі│рѕЮ', 'рІѕрІГ', 'ріарѕѕрѕЏрІѕрЅЁ', 'рѕЇрїёріЋ', 'рЅарѕЮрїЇрЅЦ', 'ріЦрїЦрѕерЅх', 'рїѕрІхрІгрІЇ', 'ріљрЅарѕГ', 'рЅарѕЏрѕѕрЅх', 'ріарѕѕрЅђрѕ▒']
    ```

    - Here is another example of performing word tokenization on a piece of plaintext using `word_tokenize` function:

    ```python
    from etnltk.tokenize.am import word_tokenize

    sample_text = """
      РђюрЅ░рѕеріЏрЇБ рЅ░рѕеріЏ!РђЮ ріарѕѕ ріљрѕГрѕ▒рЇб рІѕрІГрІўрѕ«
      рЅ│рѕфрі│рЇБ РђюріарЅцрЅх!РђЮ рЅЦрѕѕрІЇ рІерѕЂрѕѕрЅх
      рІЊрѕўрЅх рѕЇрїЃрЅИрІЇріЋ рІГрІўрІЇ рїѕрЅАрЇб
      РђюрѕЮріЉріЋ ріљрІЇ рІФрѕўрѕўрІЇ?РђЮ рІХріГрЅ░рѕ»
      рїарІерЅЂрЇб РђюріарІФрІЕрЅхрѕЮ! рЇђрїЅрѕЕ рѕ│рѕхрЅирѕЇрЇц
      рѕєрІ▒ рЅ░ріљрЇЇрЅирѕЇрЇц рІхрІ▒рѕЮ рІГрІ░рѕЏрѕЇРђЮ
      ріарѕЅ рІѕрІГрІўрѕ« рЅ│рѕфрі│рЇб рІХріГрЅ░рѕ»рѕЮрЇБ
      РђюрЅарїБрѕЮ рІФрѕ│рІЮріЊрѕЇрЇц ріЦріЋрІ░рІџрѕЁ
      рІФрІ░рѕерїѕрІЇ рІерЅ░рѕўрїБрїаріљ рѕЮрїЇрЅЦ ріарѕѕрѕЏрїЇріўрЅ▒ ріљрІЇрЇб ріарѕЂріЋрѕЮ рІѕрЅ░рЅхрЇБ
      ріЦріЋрЅЂрѕІрѕЇрЇБ рѕЏрѕГрЇБ ріарЅхріГрѕЇрЅхріЊ рЇЇрѕФрЇЇрѕг рІГрѕўрїЇрЅАрЅхрЇц рЅХрѕј рІГрѕ╗рѕѕрІІрѕЇрЇц
      рѕѕріарѕЂріЉ рїЇріЋ рѕўрІхріЃріњрЅх ріарІЮрѕѕрЅ│рѕѕрѕЂРђЮ рЅарѕЏрѕѕрЅх ріарѕхрѕерІирЅИрІЇрЇб рІѕрІГрІўрѕ«
      рЅ│рѕфрі│рѕЮ РђюрІѕрІГ ріарѕѕрѕЏрІѕрЅЁ! рѕЇрїёріЋ рЅарѕЮрїЇрЅЦ ріЦрїЦрѕерЅх рїѕрІхрІгрІЇ ріљрЅарѕГ"
      рЅарѕЏрѕѕрЅх ріарѕѕрЅђрѕ▒рЇб

    """
      
    # word tokenization
    words = word_tokenize(sample_text)

    # print all list of word:
    print(words)
    # output: ['рЅ░рѕеріЏ', 'рЅ░рѕеріЏ', 'ріарѕѕ', 'ріљрѕГрѕ▒', 'рІѕрІГрІўрѕ«', 'рЅ│рѕфрі│', 'ріарЅцрЅх', 'рЅЦрѕѕрІЇ', 'рІерѕЂрѕѕрЅх', 'ріарѕўрЅх', 'рѕЇрїЃрЅИрІЇріЋ', 'рІГрІўрІЇ', 'рїѕрЅА', 'рѕЮріЉріЋ', 'ріљрІЇ', 'рІФрѕўрѕўрІЇ', 'рІХріГрЅ░рѕ»', 'рїарІерЅЂ', 'ріарІФрІЕрЅхрѕЮ', 'рЇђрїЅрѕЕ', 'рѕ│рѕхрЅирѕЇ', 'рѕєрІ▒', 'рЅ░ріљрЇЇрЅирѕЇ', 'рІхрІ▒рѕЮ', 'рІГрІ░рѕЏрѕЇ', 'ріарѕЅ', 'рІѕрІГрІўрѕ«', 'рЅ│рѕфрі│', 'рІХріГрЅ░рѕ»рѕЮ', 'рЅарїБрѕЮ', 'рІФрѕ│рІЮріЊрѕЇ', 'ріЦріЋрІ░рІџрѕЁ', 'рІФрІ░рѕерїѕрІЇ', 'рІерЅ░рѕўрїБрїаріљ', 'рѕЮрїЇрЅЦ', 'ріарѕѕрѕЏрїЇріўрЅ▒', 'ріљрІЇ', 'ріарѕЂріЋрѕЮ', 'рІѕрЅ░рЅх', 'ріЦріЋрЅЂрѕІрѕЇ', 'рѕЏрѕГ', 'ріарЅхріГрѕЇрЅхріЊ', 'рЇЇрѕФрЇЇрѕг', 'рІГрѕўрїЇрЅАрЅх', 'рЅХрѕј', 'рІГрѕ╗рѕѕрІІрѕЇ', 'рѕѕріарѕЂріЉ', 'рїЇріЋ', 'рѕўрІхрѕђріњрЅх', 'ріарІЮрѕѕрЅ│рѕѕрѕЂ', 'рЅарѕЏрѕѕрЅх', 'ріарѕхрѕерІирЅИрІЇ', 'рІѕрІГрІўрѕ«', 'рЅ│рѕфрі│рѕЮ', 'рІѕрІГ', 'ріарѕѕрѕЏрІѕрЅЁ', 'рѕЇрїёріЋ', 'рЅарѕЮрїЇрЅЦ', 'ріЦрїЦрѕерЅх', 'рїѕрІхрІгрІЇ', 'ріљрЅарѕГ', 'рЅарѕЏрѕѕрЅх', 'ріарѕѕрЅђрѕ▒']

4. Normalization
    1. Character Level Normalization such as "`рїИ`рѕђрІГ" and "`рЇђ`рѕљрІГ"
    2. Labialized Character Normalzation such as "рѕърѕЇ`рЅ▒рІІ`рѕЇ" to "рѕърѕЇ`рЅи`рѕЇ"
    3. Short Form Expansion such as "`ріа.ріа`" to "`ріарІ▓рѕх ріарЅарЅБ`"
    4. Punctuation Normalization such as `::` to `рЇб`

    - Here is a simple example of performing normalization on a piece of plaintext using `normalize` function:

    ```python
    from etnltk.lang.am import normalize

    sample_text = """
      рѕџрІФрІЮрІФ 14рЇБ 2014 рІЊ.рѕЮ рЅарІЊрїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх рІерІЇрІГрІГрЅх рѕўрІхрѕеріГ рѕІрІГ
      рІерѕЃрїѕрѕГріЏ рЅІріЋрЅІрІјрЅй рЅхрѕГрїЅрѕЮ ріарїѕрѕЇрїЇрѕјрЅхрЇБ 
      рЅ╗рЅхрЅдрЅх (рІерІЇрІГрІГрЅх рѕўрѕѕрІІрІѕрїФ рѕ«рЅдрЅх): 
      рІерЇЁрѕЂрЇЇ рѕ░ріљрІХрЅй рѕѕрѕўрѕѕрІерЅхрЇБ рІерЅЃрѕІрЅх рЅхріГріГрѕѕріЏріљрЅхріЋ рѕѕрѕЏрѕерїІрїѕрїЦрЇБ 
      рЅарЅІріЋрЅІріЋ рѕЋрїЇрїІрЅх рѕўрѕарѕерЅх рїйрѕЉрЇјрЅйріЋ рѕѕрѕЏрІІрЅђрѕГ ріЦріЊ рѕѕрѕўрѕўрѕхрѕерЅхрЇБ 
      рѕерїЁрѕЮ рїйрѕЂрЇјрЅйріЋ рѕѕрѕЏрѕ│рїарѕГрЇБ ріаріЋрі│рѕГ рїЅрІ│рІ«рЅйріЋ рѕўрѕѕрІерЅх рІѕрІГрѕЮ рїЦрЅЁрѕЇ рѕЃрѕ│рЅЦ рѕѕрѕЏрІЇрїБрЅхрЇБ 
      ріЋрїЇрїЇрѕГріЋ рІѕрІ░ рїйрѕЂрЇЇ рѕѕрѕўрЅђрІерѕГ рІерѕџрІФрѕхрЅйрѕЅ рѕўрЅ░рїЇрЅарѕфрІФрІјрЅйріЋ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїіріљрЅ▒ рЅ░рїѕрѕЇрї╣рІІрѕЇ::
    """

    # normalization
    normalized_text = normalize(sample_text)

    # The following example shows how to print all normalized in a document:
    print(normalized_text)
    # output: рѕџрІФрІЮрІФ 14рЇБ 2014 ріарѕўрЅ░ рѕЮрѕЁрѕерЅх рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх рІерІЇрІГрІГрЅх рѕўрІхрѕеріГ рѕІрІГ
    # рІерѕђрїѕрѕГріЏ рЅІріЋрЅІрІјрЅй рЅхрѕГрїЅрѕЮ ріарїѕрѕЇрїЇрѕјрЅхрЇБ 
    # рЅ╗рЅхрЅдрЅх (рІерІЇрІГрІГрЅх рѕўрѕѕрІІрІѕрїФ рѕ«рЅдрЅх)рЇА 
    # рІерЇЁрѕЂрЇЇ рѕ░ріљрІХрЅй рѕѕрѕўрѕѕрІерЅхрЇБ рІерЅЃрѕІрЅх рЅхріГріГрѕѕріЏріљрЅхріЋ рѕѕрѕЏрѕерїІрїѕрїЦрЇБ 
    # рЅарЅІріЋрЅІріЋ рѕЁрїЇрїІрЅх рѕўрѕ░рѕерЅх рЇЁрѕЂрЇјрЅйріЋ рѕѕрѕЏрІІрЅђрѕГ ріЦріЊ рѕѕрѕўрѕўрѕхрѕерЅхрЇБ 
    # рѕерїЁрѕЮ рЇЁрѕЂрЇјрЅйріЋ рѕѕрѕЏрѕ│рїарѕГрЇБ ріаріЋрі│рѕГ рїЅрІ│рІ«рЅйріЋ рѕўрѕѕрІерЅх рІѕрІГрѕЮ рїЦрЅЁрѕЇ рѕђрѕ│рЅЦ рѕѕрѕЏрІЇрїБрЅхрЇБ 
    # ріЋрїЇрїЇрѕГріЋ рІѕрІ░ рЇЁрѕЂрЇЇ рѕѕрѕўрЅђрІерѕГ рІерѕџрІФрѕхрЅйрѕЅ рѕўрЅ░рїЇрЅарѕфрІФрІјрЅйріЋ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїіріљрЅ▒ рЅ░рїѕрѕЇрї┐рѕЇрЇб """
    ```

    - Here is another example of performing normalization on a piece of plaintext using `normalize_char`, `normalize_punct`, `normalize_labialized`, `normalize_shortened` function:

    ```python
    from etnltk.lang.am.normalizer import ( 
      normalize_labialized, 
      normalize_shortened,
      normalize_punct,
      normalize_char
    )

    # normalize labialized 
    normalized_text = normalize_labialized("ріЋрїЇрїЇрѕГріЋ рІѕрІ░ рїйрѕЂрЇЇ рѕѕрѕўрЅђрІерѕГ рІерѕџрІФрѕхрЅйрѕЅ рѕўрЅ░рїЇрЅарѕфрІФрІјрЅйріЋ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїіріљрЅ▒ рЅ░рїѕрѕЇрї╣рІІрѕЇ")
    print(normalized_text)
    # output: ріЋрїЇрїЇрѕГріЋ рІѕрІ░ рЇЁрѕЂрЇЇ рѕѕрѕўрЅђрІерѕГ рІерѕџрІФрѕхрЅйрѕЅ рѕўрЅ░рїЇрЅарѕфрІФрІјрЅйріЋ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїіріљрЅ▒ рЅ░рїѕрѕЇрї┐рѕЇ

    # normalize short forms
    normalized_text = normalize_shortened("рѕџрІФрІЮрІФ 14рЇБ 2014 рІЊ.рѕЮ рЅарІЊрїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх рІерІЇрІГрІГрЅх рѕўрІхрѕеріГ")
    print(normalized_text)
    # output: рѕџрІФрІЮрІФ 14рЇБ 2014 рІЊрѕўрЅ░ рѕЮрѕЁрѕерЅх рЅаріарїѕрѕГ рІ░рѕерїЃ рІерѕ░рІЇ рѕ░рѕФрѕй ріарѕхрЅ░рІЇрѕјрЅх рІерІЇрІГрІГрЅх рѕўрІхрѕеріГ

    # normalize punctuation
    normalized_text = normalize_punct("рѕўрЅ░рїЇрЅарѕфрІФрІјрЅйріЋ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїіріљрЅ▒ рЅ░рїѕрѕЇрї╣рІІрѕЇ::")
    print(normalized_text)
    # output: рѕўрЅ░рїЇрЅарѕфрІФрІјрЅйріЋ рѕЏрѕЇрѕЏрЅх ріарѕхрѕерѕІрїіріљрЅ▒ рЅ░рїѕрѕЇрї┐рѕЇрЇб

    # normalize characters
    normalized_text = normalize_char("рЅарЅІріЋрЅІрІЅ рѕЋрїЇрїІрЅх рѕўрѕарѕерЅх рїйрѕЉрЇјрЅйріЋ рѕЏрІІрЅђрѕГ ріЦріЊ рѕўрѕўрѕЦрѕерЅх")
    print(normalized_text)
    # output: рЅарЅІріЋрЅІрІЅ рѕЁрїЇрїІрЅх рѕўрѕ░рѕерЅх рЇЁрѕЂрЇјрЅйріЋ рѕЏрІІрЅђрѕГ ріЦріЊ рѕўрѕўрѕхрѕерЅх

## Features

- Text preprocessing functions.

    ``` python
    from etnltk.lang.am import preprocessing
    ```

    | Function | Description |
    -----------|-------------|
    | remove_whitespaces | Remove extra spaces, tabs, and new lines from a text string
    | remove_links | Remove URLs from a text string
    | remove_tags | Remove HTML tags from a text string
    | remove_emojis | Remove emojis from a text string
    | remove_email | Remove email adresses from a text string
    | remove_digits | Remove all digits from a text string
    | remove_english_chars | Remove ascii characters from a text string
    | remove_arabic_chars | Remove arabic characters and numerals from a text string
    | remove_chinese_chars | Remove chinese characters from a text string
    | remove_ethiopic_digits | Remove all ethiopic digits from a text string
    | remove_ethiopic_punct | Remove ethiopic punctuations from a text string
    | remove_non_ethiopic | Remove non ethioipc characters from a text string
    | remove_stopwords | Remove stop words
